{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "966fb29f-e5a1-4016-8610-c07040868c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('/home/gayal/ssl-project/gpatchTST')\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "from configs import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2eff4cc-7b05-4cd4-8b2d-ed37a1897a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    handlers=[\n",
    "                            logging.StreamHandler(sys.stdout),\n",
    "                            logging.FileHandler('NW_test.log')\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "BASE_PATH = '/mnt/Helium/neeraj/ssl_feature_probes'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a620f6b3-c83d-4023-9974-435cce58c860",
   "metadata": {},
   "source": [
    "### setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88e452bb-89a4-4254-8374-a38026cbf5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a833443-731c-4769-8848-ce5bc85d23be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_dirs(base_path):\n",
    "    logging.info(f\"Scanning for checkpoints in: {base_path}\")\n",
    "    checkpoint_dirs = {}\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.startswith('checkpoint') and file.endswith('.pth'):\n",
    "                checkpoint_num = int(file.split('.pth')[0].split('_')[-1])\n",
    "                checkpoint_dirs[checkpoint_num] = os.path.join(root, file)\n",
    "    logging.info(f\"Found {len(checkpoint_dirs)} checkpoints.\")\n",
    "    return checkpoint_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d64fbfa-37ee-449f-9ab5-59b8cad7e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pyt_dataset_utils import get_dataloader\n",
    "\n",
    "def save_embeddings_for_data(checkpoint, new_dataset_path, save_dir, device):\n",
    "    logging.info(f\"Processing checkpoint: {checkpoint}\")\n",
    "    config_file_path = [i for i in os.listdir(os.path.dirname(checkpoint)) if i.endswith('.yaml')][0]\n",
    "    config_file_path = os.path.join(os.path.dirname(checkpoint), config_file_path)\n",
    "    config = Config(config_file=config_file_path).get()\n",
    "\n",
    "    data_config = config['data']\n",
    "    data_config['root_path'] = new_dataset_path\n",
    "    data_config['csv_path'] = os.path.join(new_dataset_path, 'file_lengths_map.csv')\n",
    "    model_config = config['model']\n",
    "    revin = model_config['revin']\n",
    "    patch_len = model_config['patch_length']\n",
    "    stride = model_config['stride']\n",
    "\n",
    "    '''\n",
    "    SETUP DATALOADERS \n",
    "    '''\n",
    "    val_dataset = \n",
    "    val_loader = get_dataloader(type='pyt', dataset=dataset, args=args, is_val=True)\n",
    "    \n",
    "    # train_loader, val_loader, test_loader = get_tuh_dataloaders_old_splits(\n",
    "    #         data_config['root_path'],\n",
    "    #         data_config['data_path'],\n",
    "    #         data_config['csv_path'],\n",
    "    #         batch_size=data_config['batch_size'],\n",
    "    #         num_workers=data_config['num_workers'],\n",
    "    #         prefetch_factor=data_config['prefetch_factor'],\n",
    "    #         pin_memory=data_config['pin_memory'],\n",
    "    #         drop_last=False,\n",
    "    #         size=[model_config['seq_len'], \n",
    "    #               model_config['target_dim'],\n",
    "    #               model_config['patch_length']],\n",
    "    #     )\n",
    "\n",
    "    # Load pretrained model\n",
    "    model = get_patchTST_model(num_variates=data_config['n_vars'],\n",
    "                                forecast_length=model_config['target_dim'],\n",
    "                                patch_len=model_config['patch_length'],\n",
    "                                stride=model_config['stride'],\n",
    "                                num_patch=(model_config['seq_len'] - model_config['patch_length']) // model_config['stride'] + 1,\n",
    "                                n_layers=model_config['num_layers'],\n",
    "                                d_model=model_config['d_model'],\n",
    "                                n_heads=model_config['num_heads'],\n",
    "                                shared_embedding=model_config['shared_embedding'],\n",
    "                                d_ff=model_config['d_ff'],\n",
    "                                norm=model_config['norm'],\n",
    "                                attn_dropout=model_config['attn_dropout'],\n",
    "                                dropout=model_config['dropout'],\n",
    "                                activation=model_config['activation'],\n",
    "                                res_attention=model_config['res_attention'],\n",
    "                                pe=model_config['pe'],\n",
    "                                learn_pe=model_config['learn_pe'],\n",
    "                                head_dropout=model_config['head_dropout'],\n",
    "                                head_type=model_config['head_type'],\n",
    "                                use_cls_token=model_config['use_cls_token'],\n",
    "                            ).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for loader in [train_loader, val_loader, test_loader]:\n",
    "            logging.info(f\"Processing loader: {loader}\")\n",
    "            for batch in tqdm(loader, desc=f\"Extracting embeddings from {loader}\"):\n",
    "                data = batch['past_values'].to(device)\n",
    "                filename = batch['filename']\n",
    "                \n",
    "                if revin:\n",
    "                    data = revin(data, mode='norm')\n",
    "\n",
    "                input_patches, _ = create_patches(data, patch_len, stride)\n",
    "\n",
    "                output = model.backbone(input_patches) # [bs x nvars x d_model x (num_patch+1 or num_patch)]\n",
    "                output = output[:, :, :, 0] # [bs x nvars x d_model]\n",
    "\n",
    "                # save embeddings\n",
    "                for sample in range(output.shape[0]): # samples in batch\n",
    "                    sample_embeddings = output[sample].cpu().numpy() # [nvars x d_model]\n",
    "                    sample_filename = filename[sample]\n",
    "                    save_path = os.path.join(save_dir, f\"{sample_filename}.npy\")\n",
    "                    np.save(save_path, sample_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5f73aa-af49-416a-b377-e3298f8c8fe6",
   "metadata": {},
   "source": [
    "### run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0cd7d3e-7506-4e77-9f7b-512bdb4eb736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 checkpoints\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_tuh_dataloaders_old_splits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m checkpoint_save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(patch_len_save_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_num\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(checkpoint_save_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 34\u001b[0m \u001b[43msave_embeddings_for_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_save_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 15\u001b[0m, in \u001b[0;36msave_embeddings_for_data\u001b[0;34m(checkpoint, new_dataset_path, save_dir, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m patch_len \u001b[38;5;241m=\u001b[39m model_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatch_length\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m stride \u001b[38;5;241m=\u001b[39m model_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstride\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 15\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mget_tuh_dataloaders_old_splits\u001b[49m(\n\u001b[1;32m     16\u001b[0m         data_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroot_path\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     17\u001b[0m         data_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_path\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     18\u001b[0m         data_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv_path\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     19\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mdata_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     20\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39mdata_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_workers\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     21\u001b[0m         prefetch_factor\u001b[38;5;241m=\u001b[39mdata_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprefetch_factor\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     22\u001b[0m         pin_memory\u001b[38;5;241m=\u001b[39mdata_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpin_memory\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     23\u001b[0m         drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     24\u001b[0m         size\u001b[38;5;241m=\u001b[39m[model_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     25\u001b[0m               model_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_dim\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     26\u001b[0m               model_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatch_length\u001b[39m\u001b[38;5;124m'\u001b[39m]],\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Load pretrained model\u001b[39;00m\n\u001b[1;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m get_patchTST_model(num_variates\u001b[38;5;241m=\u001b[39mdata_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_vars\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     31\u001b[0m                             forecast_length\u001b[38;5;241m=\u001b[39mmodel_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_dim\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     32\u001b[0m                             patch_len\u001b[38;5;241m=\u001b[39mmodel_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatch_length\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m                             use_cls_token\u001b[38;5;241m=\u001b[39mmodel_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_cls_token\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     50\u001b[0m                         )\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_tuh_dataloaders_old_splits' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "PREPROCESSED DATA USED FOR MODEL INFERENCE\n",
    "'''\n",
    "new_data_path = '/mnt/Helium/neeraj/ssl_feature_probes/preprocessed_eeg_data'\n",
    "\n",
    "'''\n",
    "PRETRAINED MODELS \n",
    "'''\n",
    "pretrained_paths = {\n",
    "    # 1000ms = 1s\n",
    "    1: './pretrained_models/patchtst_pretrained_1s_patchlen_seq_10_sec/2025-04-17_21-01-03',\n",
    "}\n",
    "\n",
    "\n",
    "'''\n",
    "INFERENCE LOOP OVER ALL CHECKPOINTS\n",
    "'''\n",
    "for patch_len, pretrained_path in pretrained_paths.items():\n",
    "    logging.info(f\"Processing pretrained path for patch length {patch_len}: {pretrained_path}\")\n",
    "    pretrained_base_path = os.path.join(BASE_PATH, pretrained_path)\n",
    "    checkpoints = get_checkpoint_dirs(pretrained_base_path)\n",
    "    print(f\"Found {len(checkpoints)} checkpoints\")\n",
    "\n",
    "    embeddings_save_path = '/mnt/Helium/neeraj/ssl_feature_probes/saved_embeddings'\n",
    "    os.makedirs(embeddings_save_path, exist_ok=True)\n",
    "\n",
    "    patch_len_save_path = os.path.join(embeddings_save_path, f'patch_len_{patch_len}')\n",
    "    os.makedirs(patch_len_save_path, exist_ok=True)\n",
    "\n",
    "    for checkpoint_num, checkpoint_path in checkpoints.items():\n",
    "        logging.info(f\"Processing checkpoint {checkpoint_num}: {checkpoint_path}\")\n",
    "        checkpoint_save_path = os.path.join(patch_len_save_path, f'checkpoint_{checkpoint_num:03d}')\n",
    "        os.makedirs(checkpoint_save_path, exist_ok=True)\n",
    "        save_embeddings_for_data(checkpoint_path, new_data_path, checkpoint_save_path, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
